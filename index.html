<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WYV42QGLZ8"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WYV42QGLZ8');
  </script>

  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="author" content="Yen-Chen Lin">    
  <meta name="viewport" content="width=device-width, initial-scale=1">    
  <link rel="shortcut icon" href="./assets/favicon.ico">
  <meta name="description" content="Yen-Chen Lin is a Ph.D. student at MIT.">
  <meta name="keywords" content="Yen-Chen,Yen-Chen Lin,Robotics,Computer,Vision,Machine,Learning,MIT">
  <title>Yen-Chen Lin</title>

  <link rel="stylesheet" href="./assets/font.css">
  <link rel="stylesheet" href="./assets/main.css">
  <script src="./assets/main.js"></script>
  <script src="./assets/scroll.js"></script>
  <!-- Github; Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.984.0" data-gr-ext-installed="">

    <div class="outercontainer"> 
      <script src="./assets/header.js"></script>
      <header>
        <div class="container header">
          <div class="ftheader text"><a href="https://yenchenlin.me/#home">Home</a></div>    
          <div class="ftheader text"><a href="https://yenchenlin.me/#publications">Publications</a></div>
          <div class="ftheader text"><a href="https://www.notion.so/yenchenlin/Blog-a189f45f592d44259e044b87db1b92d9">Blog</a></div> 
        </div>
      </header>
      <div class="container body">

        <div class="content heading anchor" id="home">
          <div class="text info">
            <h1>Yen-Chen Lin</h1>
            <p>
            </p>
            <div>Ph.D. Student</div>
            <div>Computer Science Department</div>
            <div>MIT CSAIL</div>
            <div>Email:&nbsp;yenchenl [at] mit (dot) edu</div>
            <p>
            <span><a href="https://twitter.com/yen_chen_lin">Twitter</a></span> / 
            <span><a href="https://scholar.google.com/citations?hl=en&user=RbCKRPcAAAAJ">Google Scholar</a></span> / 
            <span><a href="https://github.com/yenchenlin">Github</a></span>
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./assets/me.jpg" alt="Photo"></div>
          <div class="text info">
            <p>I am a Ph.D. student at MIT CSAIL working with <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>. I am interested in deep learning applications.
               <br><br>
               Previously, I was lucky to have the opportunities to work with <a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a> at Uber ATG and <a href="https://aliensunmin.github.io/">Min Sun</a> at National Tsing Hua University. In addition to my research experiences, I build interesting open-source projects such as <a href="https://github.com/yenchenlin/DeepLearningFlappyBird">DeepLearningFlappyBird</a><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/yenchenlin/DeepLearningFlappyBird" data-show-count="true" aria-label="Star yenchenlin/DeepLearningFlappyBird on GitHub">Star</a>, which has been featured on Hacker News.
            </p>
          </div>
        </div>

        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Selected Publications
            </h3>
          </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/mira.gif" alt="mira"></div>
              <div class="text">
                <div class="title">MIRA: Mental Imagery for Robotic Affordances</div> 
                <div class="authors">
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Pete Florence</a></span>,
                  <span class="author">Andy Zeng</a></span>,
                  <span class="author">Jonathan T. Barron</a></span>, 
                  <span class="author">Yilun Du</span>,
                  <span class="author">Wei-Chiu Ma</span>,
                  <span class="author">Anthony Simeonov</span>,
                  <span class="author">Alberto Rodriguez</a></span>, 
                  <span class="author">Phillip Isola</a></span> 
                </div>
                <div>
                  <span class="venue">CoRL 2022</span> /
                  <span class="tag"><a href="https://www.youtube.com/watch?v=3GH-s9db5e0&ab_channel=MIRA">Video</a></span> / 
                  <span class="tag"><a href="https://arxiv.org/abs/2212.06088">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/mira/">Project Page</a></span>
                  <!--<span class="tag"><a href="https://www.icloud.com/keynote/0e86yKqA0YcWV7Q9_ehzq52Ug#nerf-supervision">Slides</a></span> /
                  <span class="tag"><a href="https://github.com/yenchenlin/nerf-supervision-public">Code</a></span> /
                  <span class="tag"><a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">Colab</a></span> -->
                </div>
                <div>
                  NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/nerf-supervision.gif" alt="nerf-supervision"></div>
              <div class="text">
                <div class="title">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</div> 
                <div class="authors">
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Pete Florence</a></span>,
                  <span class="author">Jonathan T. Barron</a></span>, 
                  <span class="author">Tsung-Yi Lin</span>,
                  <span class="author">Alberto Rodriguez</a></span>, 
                  <span class="author">Phillip Isola</a></span> 
                </div>
                <div>
                  <span class="venue">ICRA 2022</span> /
                  <span class="tag"><a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">Video</a></span> / 
                  <span class="tag"><a href="https://arxiv.org/abs/2203.01913">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/nerf-supervision/">Project Page</a></span> /
                  <span class="tag"><a href="https://www.icloud.com/keynote/0e86yKqA0YcWV7Q9_ehzq52Ug#nerf-supervision">Slides</a></span> /
                  <span class="tag"><a href="https://github.com/yenchenlin/nerf-supervision-public">Code</a></span> /
                  <span class="tag"><a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">Colab</a></span>
                </div>
                <br>
                <div>
                  Generating correspondences with Neural Radiance Fields (NeRF) enables robotic manipulation of objects (e.g., forks) that can't be reconstructed by RGB-D cameras or multi-view stereo.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/inerf.gif" alt="inerf"></div>
              <div class="text">
                <div class="title">iNeRF: Inverting Neural Radiance Fields for Pose Estimation</div> 
                <div class="authors">
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Pete Florence</a></span>,
                  <span class="author">Jonathan T. Barron</a></span>, 
                  <span class="author">Alberto Rodriguez</a></span>, 
                  <span class="author">Phillip Isola</a></span>, 
                  <span class="author">Tsung-Yi Lin</span>
                </div>
                <div>
                  <span class="venue">IROS 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2012.05877.pdf">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/inerf/">Project Page</a></span>
                </div>
                <br>
                <div>
                  Performing differentiable rendering with Neural Radiance Fields (NeRF) enables object pose estimation and camera tracking.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/dcl.png" alt="dcl"></div>
              <div class="text">
                <div class="title">Debiased Contrastive Learning</div> 
                <div class="authors">
                  <span class="author">Ching-Yao Chuang</span>,
                  <span class="author">Joshua Robinson</span>,
                  <span class="author jw">Lin Yen-Chen</span>, 
                  <span class="author">Antonio Torralba</span>, 
                  <span class="author">Stefanie Jegelka</span>
                </div>
                <div>
                  <span class="venue">Neurips 2020 <span class="highlight">Spotlight</span></span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2007.00224.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/chingyaoc/DCL">Code</a></span>
                </div>
                <br>
                <div>
                  A debiased contrastive objective that corrects for the sampling of same-label datapoints without knowledge of the true labels.
                </div>
              </div>
            </div>
                     
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/vision2action.png" alt="vision2action"></div>
              <div class="text">
                <div class="title">Learning to See before Learning to Act: Visual Pre-training for Manipulation</div> 
                <div class="authors">
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Andy Zeng</a></span>,
                  <span class="author">Shuran Song</a></span>, 
                  <span class="author">Phillip Isola</a></span>, 
                  <span class="author">Tsung-Yi Lin</span>
                </div>
                <div>
                  <span class="venue">ICRA 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/abs/2107.00646">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/vision2action/">Project Page</a></span> /
                  <span class="tag"><a href="https://ai.googleblog.com/2020/03/visual-transfer-learning-for-robotic.html">Google AI Blog</a></span>
                </div>
                <br>
                <div>
                  Transferring pre-trained vision models to perform grasping results in better sample efficiency and accuracy.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/evf.gif" alt="spotlight_nscl"></div>
              <div class="text">
                <div class="title">Experience-embedded Visual Foresight</div> 
                <div class="authors">
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Maria Bauza</a></span>,
                  <span class="author">Phillip Isola</a></span>
                </div>
                <div>
                  <span class="venue">CoRL 2019</a></span> /
                  <span class="tag"><a href="https://arxiv.org/abs/1911.05071">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/evf/">Project Page</a></span>
                </div>
                <br>
                <div>
                  Meta-learning the video prediction models allows the robot to adapt to new objects' visual dynamics. 
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/omnipush.gif" alt="spotlight_nscl"></div>
              <div class="text">
                <div class="title">Omnipush: accurate, diverse, real-world dataset of pushing dynamics with RGBD images</div> 
                <div class="authors">
                  <span class="author">Maria Bauza</a></span>,
                  <span class="author">Ferran Alet</a></span>,
                  <span class="author jw">Lin Yen-Chen</a></span>,
                  <span class="author">Maria Bauza</a></span>,
                  <span class="author">Tomas Lozano-Perez</a></span>,
                  <span class="author">Leslie P. Kaelbling</a></span>,
                  <span class="author">Phillip Isola</a></span>
                  <span class="author">Alberto Rodriguez</a></span>,
                </div>
                <div>
                  <span class="venue">IROS 2019</a></span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1910.00618.pdf">Paper</a></span> / 
                  <span class="tag"><a href="http://web.mit.edu/mcube/omnipush-dataset/index.html">Project Page</a></span> 
                </div>
                <br>
                <div>
                  A dataset for meta-learning dynamic models.
                  It consists of 250 pushes for each of 250 objects, all recorded with RGB-D camera and a high precision tracking system.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/ijcai.gif" alt="attack_RL"></div>
              <div class="text">
                <div class="title">Tactics for Adversarial Attack on Deep Reinforcement Learning Agents</div> 
                <div class="authors">
                  <span class="author jw">Yen-Chen Lin</a></span>,
                  <span class="author">Zhang-Wei Hong</a></span>,
                  <span class="author">Yuan-Hong Liao</a></span>,
                  <span class="author">Meng-Li Shih</a></span>,
                  <span class="author">Ming-Yu Liu</a></span>,
                  <span class="author">Min Sun</a></span>
                </div>
                <div>
                  <span class="venue">IJCAI 2017</a></span> /
                  <span class="tag"><a href="https://arxiv.org/abs/1703.06748">Paper</a></span> / 
                  <span class="tag"><a href="http://yenchenlin.me/adversarial_attack_RL/">Project Page</a></span>
                </div>
                <br>
                <div>
                  Strategies for adversarial attacks on deep RL agents.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/cvpr2017.png" alt="spotlight_nscl"></div>
              <div class="text">
                <div class="title">Deep 360 Pilot: Learning a Deep Agent for Piloting through 360&deg Sports Videos</div> 
                <div class="authors">
                  <span class="author jw">Yen-Chen Lin*</a></span>,
                  <span class="author">Hou-Ning Hu*</a></span>,
                  <span class="author">Ming-Yu Liu</a></span>,
                  <span class="author">Hsien-Tzu Cheng</a></span>,
                  <span class="author">Yung-Ju Chang</a></span>,
                  <span class="author">Min Sun</a></span>
                </div>
                <div>
                  <span class="venue">CVPR 2017</a></span>
                </div>
                <div>
                  <span class="tag"><a href="https://arxiv.org/abs/1705.01759">Paper</a></span> / 
                  <span class="tag"><a href="http://aliensunmin.github.io/project/360video/">Project Page</a></span>
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <br>
                <div>
                  An agent that learns to guide users where to look in 360&deg sports videos.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./assets/chi2017.png" alt="spotlight_nscl"></div>
              <div class="text">
                <div class="title">Tell Me Where to Look: Investigating Ways for Assisting Focus in 360&deg Video</div> 
                <div class="authors">
                  <span class="author jw">Yen-Chen Lin</a></span>,
                  <span class="author">Yung-Ju Chang</a></span>,
                  <span class="author">Hou-Ning Hu</a></span>,
                  <span class="author">Hsien-Tzu Cheng</a></span>,
                  <span class="author">Chi-Wen Huang</a></span>,
                  <span class="author">Min Sun</a></span>
                </div>
                <div>
                  <span class="venue">CHI 2017</a></span>
                </div>
                <div>
                  <span class="tag"><a href="https://dl.acm.org/doi/abs/10.1145/3025453.3025757">Paper</a></span> / 
                  <span class="tag"><a href="http://aliensunmin.github.io/project/360video-study/">Project Page</a></span>
                </div>
                <br>
                <div>
                  A study about how to assist users when watching 360&deg videos.
                </div>
              </div>
            </div>

        </div>  <!-- content -->
      </div> <!-- container -->
    </div> <!-- outer container -->
    <script>showPubs(0);</script>
    <script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>

  


</body></html>
