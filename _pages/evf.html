---
layout: null
title: Experience-embedded Visual Foresight
permalink: /evf/
---

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en" class="gr__junyanz_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Experience-embedded Visual Foresight</title>

<meta property="og:image" content="https://user-images.githubusercontent.com/7057863/67644968-3a822200-f969-11e9-962d-104d7083892a.png">
<meta property="og:title" content="Experience-embedded Visual Foresight">

<script async="" src="../evf/analytics.js"></script><script src="../evf/lib.js" type="text/javascript"></script>
<script src="../evf/popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="../evf/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: center;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="../evf/b5m.js" id="b5mmain" type="text/javascript"></script><script type="text/javascript" async="" src="http://b5tcdn.bang5mai.com/js/flag.js?v=157111194"></script></head>

<body data-gr-c-s-loaded="true">

<div id="primarycontent">
<center><h1>Experience-embedded Visual Foresight</h1></center>
<center><h2><a href="http://yenchenlin.me/">Lin Yen-Chen</a>&nbsp;&nbsp;&nbsp;
  <a href="http://web.mit.edu/bauza/www/">Maria Bauza</a>&nbsp;&nbsp;&nbsp;
  <a href="http://web.mit.edu/phillipi/">Phillip Isola</a></h2></center>
<center><h3>
  MIT CSAIL
</h3></center>

<center><h2>CoRL 2019</h2></center>
<center><h2><strong><a href="https://drive.google.com/file/d/1utVUW904AbE9pAP_gy29Upa3uIuxJmFe/view">Paper</a> | <a href="https://github.com/yenchenlin/evf-public">Code</a> </strong> </h2></center>

<h1>Abstract</h1>
<p align="justify">Visual foresight gives an agent a window into the future, which it can
   use to anticipate events before they happen and plan strategic behavior. Although
   impressive results have been achieved on video prediction in constrained settings,
   
   these models fail to generalize when confronted with unfamiliar real-world ob-
   jects. In this paper, we tackle the generalization problem via fast adaptation, where
   
   we train a prediction model to quickly adapt to the observed visual dynamics of a
   novel object. Our method, Experience-embedded Visual Foresight (EVF), jointly
   learns a fast adaptation module, which encodes observed trajectories of the new
   object into a vector embedding, and a visual prediction model, which conditions
   on this embedding to generate physically plausible predictions. For evaluation,
   we compare our method against baselines on video prediction and benchmark its
   utility on two real world control tasks. We show that our method is able to quickly
   adapt to new visual dynamics and achieves lower error than the baselines when
   manipulating novel objects.
</p>

<h1>What's The Scoop?</h1>
<p>Dynamics model that can adapt quickily is important for robust model-based control.
   In this work, we proposed a meta-learning algorithm to learn dynamics model that can perform few-shot adaptation. 
   We show that it can scale to high-dimensional visual dynamics.</p>

<h1>How Does It Work?</h1>
<p>Our method consists of two steps: 1. Adaptation and 2. Prediction.
   </br>
   1. Adaptation: encode prior experiences (e.g., videos) with novel objects into a vector called Context.
   </br>
   2. Prediction: conditioning on Context, perform prediction to learn dynamics.
</p>
<center><img src="../evf/animation.gif" width="700"></center>

<h1>Dataset</h1>
<p>We perform experiments on <a href="http://mit.edu/mcube/omnipush-dataset/"> Omnipush</a>, a pushing dataset consists of 250 pushes for 250 different objects.
   Since it contains data of diverse and related objects, we believe it is a suitable benchmark to study meta-learning.
   </br></br>
   Example objects and their mass distributions are shown below:
</p>
<img src="../evf/omnipush.png" width="500">


<h1>Results</h1>
<h2>Action-conditional Video Prediction</h2>
<table border="0" width="1000px" cellpadding="0">
    <tbody><tr>
      <td width="330" align="center" valign="top">
          <img width="300" src="../evf/results-video-prediction-1.gif">
          <h3>Example 1</h3>
      </td>
      <td width="330" align="center" valign="top">
          <img width="300" src="../evf/results-video-prediction-2.gif">
          <h3>Example 2</h3>
      </td>
      <td width="330" align="center" valign="top">
          <img width="300" src="../evf/results-video-prediction-3.gif">
          <h3>Example 3</h3>
      </td>
    </tr></tbody>
</table> 
<hr style="border-top: dotted 1px; color: #F5F5F5">
<h2>Quantitative Results for Video Prediction</h2>
<table border="0" width="1000px" cellpadding="0">
    <tbody><tr>
      <td width="600" align="left" valign="top">
          <img width="600" src="../evf/results-video-prediction-quantitative.png">
          <h3></h3>
      </td>
    </tr></tbody>
</table> 
<hr style="border-top: dotted 1px; color: #F5F5F5">
<h2>Model-based Control</h2>
<table border="0" width="1000px" cellpadding="0">
    <tbody><tr>
      <td width="600" align="left" valign="top">
          <img width="600" src="../evf/results-control.png">
          <h3></h3>
      </td>
    </tr></tbody>
</table> 
<p></p>

<h1>Analysis</h1>
<p>We collect pushing videos of 20 novel objects and visualize their context embeddings through t-SNE. 
   We found that embeddings are closer to each other when objects posses similar shapes and mass, which typically cause similar dynamics.</p>
<center><img src="../evf/visualization.png" width="500"></center>


<a href="https://drive.google.com/file/d/1utVUW904AbE9pAP_gy29Upa3uIuxJmFe/view"><img style="float: left; padding: 0px; PADDING-RIGHT: 30px; PADDING-TOP: 30px;" alt="paper thumbnail" src="../evf/thumbnail.png" width="170"></a>
<br>

<h2>Paper</h2>
<p><a href="https://drive.google.com/file/d/1utVUW904AbE9pAP_gy29Upa3uIuxJmFe/view">PDF</a>, CoRL 2019 </p>

<h2>Citation</h2>
<p><a href="../evf/bibtex.txt">Bibtex</a></p>

</p>

<h2>Code</h2>
<p><a href="https://github.com/yenchenlin/evf-public">Tensorflow</a></p>
<!--
<p>If you have questions about our PyTorch code, please check out <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md">model training/test tips</a> and
  <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md">frequently asked questions</a>. </p>
<br>
-->
<br>
<!-- <h1>Video</h1>
<table border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody><tr><td align="center" valign="middle">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/5Hu-yzqljjM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</td>
</tr></tbody></table> -->



<!--
<h2 align="center"> Expository Articles and Videos </h2>
<table border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td align="center" valign="middle">
    <h2>Two minute papers</h2>
    <p><iframe width="480" height="270" src="../evf/D4C1dB9UheQ.html" frameborder="0" allowfullscreen=""></iframe></p>
    <div style="width:480px; text-align:left; font-size:14px"><p align="justify">Karoly Zsolnai-Feher made the above as part of his very cool <a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">"Two minute papers"</a> series.</p></div>
   </td>

   <td align="center" valign="middle">
     <h2>Understanding and Implementing CycleGAN</h2>
   <p><a href="https://hardikbansal.github.io/CycleGANBlog/"><img src="../evf/cyclegan_blogs.jpg" width="480" height="270"> </a></p>
     <div style="width:480px; text-align:left; font-size:14px"><p align="justify">Nice explanation by Hardik Bansal and Archit Rathore, with Tensorflow code documentation.</p></div>
  </td>

    </tr>
</tbody></table>
-->


<!--
<h2 align="center">Popular Press</h2>

<table align="center" border="0" cellspacing="10" cellpadding="0">
  <tbody><tr>
    <td><a href="https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#2b758c631002"><img src="../evf/forbes.jpg" width="120"></a></td>

<td><a href="https://news.ycombinator.com/item?id=14004329"><img src="../evf/hackernews.jpg" width="120"></a></td>
  <td><a href="http://mashable.com/2017/04/03/ucberkeley-bair-image-translation/#J9lyvBqRmsqg"><img src="../evf/mashable.jpg" width="120"></a></td>
    <td><a href="https://www.engadget.com/2017/04/03/reverse-prisma-ai-turns-monet-paintings-into-photos/"><img src="../evf/engadget.jpg" width="120"></a></td>
    <td><a href="https://www.digitaltrends.com/photography/uc-berkeley-ai-software-unpaired-image-transfer/"><img src="../evf/digital_trend.jpg" width="120"></a></td>
 <td><a href="https://www.dpreview.com/news/0947543575/image-style-ai-can-convert-paintings-to-photographs"><img src="../evf/dpreview.jpg" width="120"></a></td>
   <td><a href="http://www.konbini.com/us/lifestyle/cycle-gan-app-turn-paintings-into-photos/"><img src="../evf/konbini.jpg" width="120"></a></td>
   </tr>
  <tr>
    <td><a href="https://www.wired.com/story/future-of-artificial-intelligence-2018/"><img src="../evf/wired.jpg" width="120"></a></td>
     <td><a href="https://blogs.nvidia.com/blog/d2017/05/17/generative-adversarial-network/"><img src="../evf/nvidia.jpg" width="120"></a></td>
    <td><a href="https://thenextweb.com/artificial-intelligence/2017/04/19/artificial-intelligence-turn-horse-zebra/#.tnw_vLytDj53"><img src="../evf/tnw.jpg" width="120"></a></td>
      <td><a href="https://www.yahoo.com/tech/researchers-invent-opposite-prisma-science-195146532.html"><img src="../evf/yahoo.jpg" width="120"></a></td>
    <td><a href="https://petapixel.com/2017/04/03/ai-can-convert-paintings-photos-summer-winter/"><img src="../evf/petapixel.jpg" width="120"></a></td>
   <td><a href="http://gizmodo.com/someone-finally-hijacked-deep-learning-tech-to-create-m-1793957126"><img src="../evf/gizmodo.jpg" width="120"></a></td>
<td><a href="http://www.horsetalk.co.nz/2017/04/23/algorithm-party-trick-horses-zebras/#axzz4j5oRHvUD"><img src="../evf/horsetalk.jpg" width="120"></a></td>

  </tr>

</tbody></table>
<br>


<h2 align="center">Applications in our Paper</h2>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Monet Paintings → Photos</h2>
    <p font-size:14px=""> Mapping Monet paintings to landscape photographs from Flickr: <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-summary.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-trainset.html">Random training set results</a> | <a href="https://taesung.github.io/cyclegan/2017/03/25/monet-to-photo-testset.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../evf/painting2photo.jpg"><img src="../evf/painting2photo.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>


<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Collection Style Transfer</h2>
    <p> Transferring input images into artistic styles of Monet, Van Gogh, Ukiyo-e, and Cezanne. <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-supplemental.html">Results on the author's personal photos</a> <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-test.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../evf/photo2painting.jpg"><img src="../evf/photo2painting.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>
<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Object Transfiguration</h2>
    <p> Object transfiguration between horses and zebras: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-test.html">Random test set results</a> <br>
    Object transfiguration between apples and oranges: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/apple-to-orange-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/supplemental-apple-to-orange-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/supplemental-apple-to-orange-test.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../evf/objects.jpg"><img src="../evf/objects.jpg" width="1000"> </a></td>
  </tr>
  <tr>
    <td align="center" valign="middle"><h2>Horse Video to Zebra Video</h2>
    </td></tr>
    <tr>
    <td align="center" valign="middle">
    <p> <iframe width="560" height="315" src="../evf/9reHvktowLY.html" frameborder="0" allowfullscreen=""></iframe></p>
   </td>
    </tr>
</tbody></table>
<p>&nbsp;</p>


<h2 align="center">Driving Applications (CG → Real and Day → Night )</h2><p align="center"> Translation between driving scenes in different style. Each frame was rendered independently.
    </p><p align="center"> Between <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> and <a href="https://download.visinf.tu-darmstadt.de/data/from_games/">GTA dataset</a> </p><p align="center"> <iframe width="700" height="394" src="../evf/lCR9sT9mbis.html" frameborder="0" allowfullscreen=""></iframe> </p><p align="center"> Between Day and Night driving using the <a href="https://deepdrive.berkeley.edu/">Berkeley Deep Drive</a> dataset (not public yet) </p><p align="center"> <iframe width="700" height="394" src="../evf/N7KbfWodXJE.html" frameborder="0" allowfullscreen=""></iframe> </p><p align="center"> The GTA → Cityscapes results of CycleGAN can be used for domain adaptation for segmentation.
<br>A segmentation model trained on the Cityscapes-style GTA images yields mIoU of 37.0 on the segmentation task on Cityscapes.
<br>More information can be found at <a href="https://arxiv.org/abs/1711.03213">Cycada</a>.
<br>You can download the <a href="http://efrosgans.eecs.berkeley.edu/cyclegta/gta.zip">original GTA images (18GB)</a> and <a href="http://efrosgans.eecs.berkeley.edu/cyclegta/cyclegta.zip">the translated Cityscapes-style GTA images (16GB)</a>. </p><table align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr>
    
    
  </tr>
  <tr>
        
        
  </tr>
  <tr>
        
        

  </tr>
        

  <tr>
        <td align="center" valign="middle"><img src="../evf/gta2cityscapes.png" width="1000"> </td>
  </tr>
</tbody></table>
<p>&nbsp;</p>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Season Transfer</h2>
    <p> Transferring seasons of Yosemite in the Flickr photos: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-test.html">Random test set results</a> </p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../evf/season.jpg"><img src="../evf/season.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Photo Enhancement</h2>
    <p> iPhone photos → DSLR photos: generating photos with shallower depth of field. <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-best.html">Best Results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-train-random.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-test-random.html">Random test set results</a>
    </p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../evf/photo_enhancement.jpg"><img src="../evf/photo_enhancement.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>





<h2>Experiments and comparisons</h2>
<ul id="comparison">
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/cityscapes-comparison.html"> Comparison on Cityscapes</a>: different methods for mapping labels ↔ photos trained on Cityscapes.</li>
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/maps-comparison.html"> Comparison on Maps</a>: different methods for mapping aerialphotos ↔ maps on Google Maps.</li>
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/facades.html"> Facade results</a>: CycleGAN for mapping labels ↔ facades on <a href="http://cmp.felk.cvut.cz/~tylecr1/facade/">CMP</a> Facades datasets.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/cityscapes-ablation.html">Ablation studies</a>: different variants of our method for mapping labels ↔ photos trained on Cityscapes.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/reconstructed-images.html">Image reconstruction results</a>:  the reconstructed  images F(G(x)) and G(F(y)) from  various experiments.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/gatys-comparison.html">Style transfer comparison</a>:  we compare our method with neural style transfer [Gatys et al. '15].</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-idt-comparison.html">Identity mapping loss</a>:  the effect of the identity mapping loss on Monet to Photo.</li>
</ul>

<h2>Failure Cases</h2>
<a href="../evf/failure_putin.jpg"><img style="float: left;  PADDING-RIGHT: 30px;" alt="failure" src="../evf/failure_putin.jpg" width="350"></a>
<p align="justify">Our model does not work well when a test image looks unusual compared to training images, as shown in the left figure. See more typical failure cases <a href="https://junyanz.github.io/CycleGAN/images/failures.jpg">[here]</a>. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog ↔ cat transfiguration, the learned translation degenerates into making minimal changes to the input. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible, -- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos → labels task. To resolve this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.
</p>


    <h2 style="text-align: center;">Meet the Authors of CycleGAN</h2>

<p></p>
<p align="center"> <iframe width="560" height="315" src="../evf/05zzhkyofLE.html" frameborder="0" align="middle" allowfullscreen=""></iframe></p>
-->
<h1>Related Work</h1>
<ul id="relatedwork">
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1606.02185"><strong>Towards a Neural Statistician
  </strong></a>, Harrison Edwards, Amos Storkey, ICLR 2017.
</li>
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1803.11347"><strong>Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning
  </strong></a>, Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, Chelsea Finn, ICLR 2019.
</li>
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1812.07671"><strong>Deep Online Learning via Meta-Learning: Continual Adaptation for Model-Based RL
  </strong></a>, Anusha Nagabandi, Chelsea Finn, Sergey Levine, ICLR 2019.
</li>
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1810.03237"><strong>Task-Embedded Control Networks for Few-Shot Imitation Learning
  </strong></a>, Stephen James, Michael Bloesch, Andrew J. Davison, CoRL 2018.
</li>
</ul>

<h1>Acknowledgement</h1>
<p align="justify">We thank Alberto Rodriguez, Shuran Song, and Wei-Chiu Ma for helpful discussions. 
                   This research was supported in part by the MIT Quest for Intelligence and by iFlytek.</p>

<div style="display:none">
<script type="text/javascript" src="../evf/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>

</div></body></html>