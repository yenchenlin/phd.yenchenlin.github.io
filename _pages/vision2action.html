---
layout: null
title: Vision2Action
permalink: /vision2action/
---

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en" class="gr__junyanz_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Vision2Action</title>

<meta property="og:image" content="https://user-images.githubusercontent.com/7057863/67644968-3a822200-f969-11e9-962d-104d7083892a.png">
<meta property="og:title" content="Learning to See before Learning to Act: Visual Pre-training for Manipulation">

<script async="" src="../CycleGAN Project Page_files/analytics.js"></script><script src="../CycleGAN Project Page_files/lib.js" type="text/javascript"></script>
<script src="../CycleGAN Project Page_files/popup.js" type="text/javascript"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="../CycleGAN Project Page_files/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: center;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="../CycleGAN Project Page_files/b5m.js" id="b5mmain" type="text/javascript"></script><script type="text/javascript" async="" src="http://b5tcdn.bang5mai.com/js/flag.js?v=157111194"></script></head>

<body data-gr-c-s-loaded="true">

<div id="primarycontent">
<center><h1>Learning to See before Learning to Act: Visual Pre-training for Manipulation</h1></center>
<center><h2><a href="http://yenchenlin.me/">Lin Yen-Chen<sup>1,2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="http://andyzeng.github.io/">Andy Zeng<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://shurans.github.io/index.html">Shuran Song<sup>1,3</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="http://web.mit.edu/phillipi/">Phillip Isola<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://vision.cornell.edu/se3/people/tsung-yi-lin/">Tsung-Yi Lin<sup>1</sup></a></h2></center>
<center><h3>
  <sup>1</sup> Google AI&nbsp;&nbsp;&nbsp;
  <sup>2</sup> MIT CSAIL&nbsp;&nbsp;&nbsp;
  <sup>3</sup> Columbia University
</h3></center>

<center><h2><strong><a href="https://arxiv.org/pdf/1703.10593.pdf">Paper</a> | <a href="https://github.com/yenchenlin/vision2action">Code</a>  | <a href="https://www.youtube.com/embed/5Hu-yzqljjM">Video</a> </strong> </h2></center>
<center><h2></h2></center>

<center><img style="float:right; PADDING-TOP: 15px;" src="../CycleGAN Project Page_files/overview.png" width="500"></center>
<center><h2>Abstract</h2></center>
<p align="justify">Does having visual priors (e.g. the ability to detect objects)  facilitate  learning  to  perform  vision-based  manipula-tion  (e.g.  picking  up  objects)?  We  study  this problem  under the  framework  of  transfer  learning,  where  the  model  is  first trained on a passive vision task, and adapted to perform an active manipulation task.
  We  find  that  pre-training  on  vision  tasks  significantly  improves  generalization  and  sample  efficiency  for  learning  to manipulate  objects.  However,  realizing  these  gains  require scareful  selection  of  which  parts  of  the  model  to  transfer.  Our key  insight  is  that  outputs  of  standard  vision  models  highly correlate with affordance maps commonly used in manipulation. Therefore,  we  explore  directly  transferring  model  parameters from  vision  networks  to  affordance  prediction  networks,  and show  that  this  can  result  in  successful  zero-shot  adaptation, where  a  robot  can  pick  up  certain  objects  with  zero  robotic experience. With just a small amount of robotic experience, we can  further  fine-tune  the  affordance  model  to  achieve  better results.  With  just  10  minutes  of  suction  experience  or  1  hour of  grasping  experience,  our  method  achieves∼80% success rate  at  picking  up  hold-out  objects.
</p>


<h1>What Is Affordance?</h1>
<p>In our work, we define affordance as probability of picking success. Given an RGB-D image, we predict pixel-wise affordance which represents the success rate of performing the corresponding motion primitive at that pixel’s projected 3D position. 
   The motion primitive is then executed by the robot at the position with the highest affordance value.
</p>
<center><img src="../CycleGAN Project Page_files/affordance.png" width="800"></center>

<h1>How to Learn Affordance?</h1>
<p>We can let robot measure the success of grasping automatically after each trial and train itself with self-supervised learning.</p>
<center><img src="../CycleGAN Project Page_files/suction_training.gif" width="400"></center>
<p>However, learning affordance leads to two problems: 1) bad sample efficiency and 2) limited generalization due to limited training objects.

<h1>How to Learn Affordance with Better Generalization and Sample Efficiency</h1>
<p>We propose to transfer knowledge learned from static computer vision dataset to improve affordance model's generalization and sample efficiency.
   However, simply transferring ImageNet features doesn't speed up the learning of affordance. The key problem is that by randomly initializing the head of the affordance model, the resulting policy, even with pre-trained latent features, still randomly explores the environments and thus fails to collect useful supervisory signals. 
   Therefore, we note that it's important to transfer the entire vision model, including both features from the backbone and the visual predictions from the head, to initialize the affordance model. In this case, the resulting initial policy is simply the pre-trained vision model. We show that such vision-guided exploration greatly reduces the number of interactions needed to acquire a new manipulation skill and lead to better generalization.</p>
<center><video width="600" controls="controls" autoplay="autoplay" loop="loop" onstart="this.play();" onended="this.play();" muted><source src="../CycleGAN Project Page_files/explanation.mp4" type="video/mp4"></video></center>


<h1>Results</h1>
<h2>Generalization with or without Visual Pre-training</h2>
<table border="0" width="1000px" cellpadding="0">
    <tbody><tr>
      <td width="500" align="center" valign="top">
        <img width="500" src="../CycleGAN Project Page_files/suction_visual_pretraining.gif">
        <h3> Ours (visual pre-training): <br>10 mins training, 80% success rate on unseen objects</h3>
      </td>
    
      <td width="500" align="center" valign="top">
        <img width="500" src="../CycleGAN Project Page_files/suction_from_scratch.gif">
        <h3> Baseline (random initialization): <br>60 mins training, 20% success rate on unseen objects</h3>
      </td>  
    </tr></tbody>
</table> 
<hr style="border-top: dotted 1px; color: #F5F5F5">
<h2>More Results with Visual Pre-training</h2>
<table border="0" width="1000px" cellpadding="0">
    <tbody><tr>
      <td width="333px" align="center" valign="top">
          <img width="333" src="../CycleGAN Project Page_files/grasping_transparent.gif">
          <h3>Grasping Transparent Objects <br> (No Transparent Objects in Training Set)</h3>
      </td>

      <td width="333px" align="center" valign="top">
          <img width="333" src="../CycleGAN Project Page_files/grasping_unseen.gif">
          <h3>Grasping Unseen Objects <br> (Testing)</h3>
      </td>

      <td width="333px" align="center" valign="top">
          <img width="333" src="../CycleGAN Project Page_files/grasping_seen.gif">
          <h3>Grasping Seen Objects <br> (Training)</h3>
      </td>
    </tr></tbody>
</table>
<p></p>

<h1>Analysis</h1>
<p>We show the affordances of suction predicted by different models given input from (a). (b) Random refers to a randomly initialized model. (c) ImageNet is a model with backbone pre-trained on ImageNet and randomly initialized head. (d) Normal refers to a model pre-trained to detect pixels with normal close to anti-gravity axis. (e) COCO is the modified RPN model. (f) Suction is a converged model learned from robot-environment interactions.</p>
<center><img src="../CycleGAN Project Page_files/visualization.png" width="800"></center>


<a href="https://arxiv.org/pdf/1703.10593.pdf"><img style="float: left; padding: 0px; PADDING-RIGHT: 30px; PADDING-TOP: 30px;" alt="paper thumbnail" src="../CycleGAN Project Page_files/thumbnail.png" width="170"></a>
<br>

<h2>Paper</h2>
<p><a href="https://arxiv.org/pdf/1703.10593.pdf">arxiv 1703.10593</a>,  2019. </p>

<h2>Citation</h2>
<p>Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, and Tsung-Yi Lin. "Learning to See before Learning to Act: Visual Pre-training for Manipulation", 2020.
<a href="https://junyanz.github.io/CycleGAN/vision2action.txt">Bibtex</a>

</p>

<h2>Code</h2>
<p><a href="https://github.com/yenchenlin/vision2action">PyTorch</a></p>
<!--
<p>If you have questions about our PyTorch code, please check out <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md">model training/test tips</a> and
  <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md">frequently asked questions</a>. </p>
<br>
-->
<h1>Video</h1>
<table border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody><tr><td align="center" valign="middle">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/5Hu-yzqljjM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</td>
</tr></tbody></table>

<!--
<h2 align="center"> Expository Articles and Videos </h2>
<table border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td align="center" valign="middle">
    <h2>Two minute papers</h2>
    <p><iframe width="480" height="270" src="../CycleGAN Project Page_files/D4C1dB9UheQ.html" frameborder="0" allowfullscreen=""></iframe></p>
    <div style="width:480px; text-align:left; font-size:14px"><p align="justify">Karoly Zsolnai-Feher made the above as part of his very cool <a href="https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg">"Two minute papers"</a> series.</p></div>
   </td>

   <td align="center" valign="middle">
     <h2>Understanding and Implementing CycleGAN</h2>
   <p><a href="https://hardikbansal.github.io/CycleGANBlog/"><img src="../CycleGAN Project Page_files/cyclegan_blogs.jpg" width="480" height="270"> </a></p>
     <div style="width:480px; text-align:left; font-size:14px"><p align="justify">Nice explanation by Hardik Bansal and Archit Rathore, with Tensorflow code documentation.</p></div>
  </td>

    </tr>
</tbody></table>
-->


<!--
<h2 align="center">Popular Press</h2>

<table align="center" border="0" cellspacing="10" cellpadding="0">
  <tbody><tr>
    <td><a href="https://www.forbes.com/sites/quora/2017/07/21/whats-next-for-deep-learning/#2b758c631002"><img src="../CycleGAN Project Page_files/forbes.jpg" width="120"></a></td>

<td><a href="https://news.ycombinator.com/item?id=14004329"><img src="../CycleGAN Project Page_files/hackernews.jpg" width="120"></a></td>
  <td><a href="http://mashable.com/2017/04/03/ucberkeley-bair-image-translation/#J9lyvBqRmsqg"><img src="../CycleGAN Project Page_files/mashable.jpg" width="120"></a></td>
    <td><a href="https://www.engadget.com/2017/04/03/reverse-prisma-ai-turns-monet-paintings-into-photos/"><img src="../CycleGAN Project Page_files/engadget.jpg" width="120"></a></td>
    <td><a href="https://www.digitaltrends.com/photography/uc-berkeley-ai-software-unpaired-image-transfer/"><img src="../CycleGAN Project Page_files/digital_trend.jpg" width="120"></a></td>
 <td><a href="https://www.dpreview.com/news/0947543575/image-style-ai-can-convert-paintings-to-photographs"><img src="../CycleGAN Project Page_files/dpreview.jpg" width="120"></a></td>
   <td><a href="http://www.konbini.com/us/lifestyle/cycle-gan-app-turn-paintings-into-photos/"><img src="../CycleGAN Project Page_files/konbini.jpg" width="120"></a></td>
   </tr>
  <tr>
    <td><a href="https://www.wired.com/story/future-of-artificial-intelligence-2018/"><img src="../CycleGAN Project Page_files/wired.jpg" width="120"></a></td>
     <td><a href="https://blogs.nvidia.com/blog/d2017/05/17/generative-adversarial-network/"><img src="../CycleGAN Project Page_files/nvidia.jpg" width="120"></a></td>
    <td><a href="https://thenextweb.com/artificial-intelligence/2017/04/19/artificial-intelligence-turn-horse-zebra/#.tnw_vLytDj53"><img src="../CycleGAN Project Page_files/tnw.jpg" width="120"></a></td>
      <td><a href="https://www.yahoo.com/tech/researchers-invent-opposite-prisma-science-195146532.html"><img src="../CycleGAN Project Page_files/yahoo.jpg" width="120"></a></td>
    <td><a href="https://petapixel.com/2017/04/03/ai-can-convert-paintings-photos-summer-winter/"><img src="../CycleGAN Project Page_files/petapixel.jpg" width="120"></a></td>
   <td><a href="http://gizmodo.com/someone-finally-hijacked-deep-learning-tech-to-create-m-1793957126"><img src="../CycleGAN Project Page_files/gizmodo.jpg" width="120"></a></td>
<td><a href="http://www.horsetalk.co.nz/2017/04/23/algorithm-party-trick-horses-zebras/#axzz4j5oRHvUD"><img src="../CycleGAN Project Page_files/horsetalk.jpg" width="120"></a></td>

  </tr>

</tbody></table>
<br>


<h2 align="center">Applications in our Paper</h2>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Monet Paintings → Photos</h2>
    <p font-size:14px=""> Mapping Monet paintings to landscape photographs from Flickr: <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-summary.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-trainset.html">Random training set results</a> | <a href="https://taesung.github.io/cyclegan/2017/03/25/monet-to-photo-testset.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../CycleGAN Project Page_files/painting2photo.jpg"><img src="../CycleGAN Project Page_files/painting2photo.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>


<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Collection Style Transfer</h2>
    <p> Transferring input images into artistic styles of Monet, Van Gogh, Ukiyo-e, and Cezanne. <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-supplemental.html">Results on the author's personal photos</a> <br> <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/style-transfer-test.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../CycleGAN Project Page_files/photo2painting.jpg"><img src="../CycleGAN Project Page_files/photo2painting.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>
<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Object Transfiguration</h2>
    <p> Object transfiguration between horses and zebras: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/horse-to-zebra-supplemental-test.html">Random test set results</a> <br>
    Object transfiguration between apples and oranges: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/apple-to-orange-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/supplemental-apple-to-orange-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/supplemental-apple-to-orange-test.html">Random test set results</a></p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../CycleGAN Project Page_files/objects.jpg"><img src="../CycleGAN Project Page_files/objects.jpg" width="1000"> </a></td>
  </tr>
  <tr>
    <td align="center" valign="middle"><h2>Horse Video to Zebra Video</h2>
    </td></tr>
    <tr>
    <td align="center" valign="middle">
    <p> <iframe width="560" height="315" src="../CycleGAN Project Page_files/9reHvktowLY.html" frameborder="0" allowfullscreen=""></iframe></p>
   </td>
    </tr>
</tbody></table>
<p>&nbsp;</p>


<h2 align="center">Driving Applications (CG → Real and Day → Night )</h2><p align="center"> Translation between driving scenes in different style. Each frame was rendered independently.
    </p><p align="center"> Between <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> and <a href="https://download.visinf.tu-darmstadt.de/data/from_games/">GTA dataset</a> </p><p align="center"> <iframe width="700" height="394" src="../CycleGAN Project Page_files/lCR9sT9mbis.html" frameborder="0" allowfullscreen=""></iframe> </p><p align="center"> Between Day and Night driving using the <a href="https://deepdrive.berkeley.edu/">Berkeley Deep Drive</a> dataset (not public yet) </p><p align="center"> <iframe width="700" height="394" src="../CycleGAN Project Page_files/N7KbfWodXJE.html" frameborder="0" allowfullscreen=""></iframe> </p><p align="center"> The GTA → Cityscapes results of CycleGAN can be used for domain adaptation for segmentation.
<br>A segmentation model trained on the Cityscapes-style GTA images yields mIoU of 37.0 on the segmentation task on Cityscapes.
<br>More information can be found at <a href="https://arxiv.org/abs/1711.03213">Cycada</a>.
<br>You can download the <a href="http://efrosgans.eecs.berkeley.edu/cyclegta/gta.zip">original GTA images (18GB)</a> and <a href="http://efrosgans.eecs.berkeley.edu/cyclegta/cyclegta.zip">the translated Cityscapes-style GTA images (16GB)</a>. </p><table align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr>
    
    
  </tr>
  <tr>
        
        
  </tr>
  <tr>
        
        

  </tr>
        

  <tr>
        <td align="center" valign="middle"><img src="../CycleGAN Project Page_files/gta2cityscapes.png" width="1000"> </td>
  </tr>
</tbody></table>
<p>&nbsp;</p>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Season Transfer</h2>
    <p> Transferring seasons of Yosemite in the Flickr photos: <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-best.html">Best results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-train.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/yosemite-supplemental-test.html">Random test set results</a> </p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../CycleGAN Project Page_files/season.jpg"><img src="../CycleGAN Project Page_files/season.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>

<table border="0" cellspacing="0" cellpadding="0">
  <tbody><tr>
    <td align="center" valign="middle"><h2>Photo Enhancement</h2>
    <p> iPhone photos → DSLR photos: generating photos with shallower depth of field. <br><a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-best.html">Best Results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-train-random.html">Random training set results</a> | <a href="https://taesungp.github.io/cyclegan/2017/03/25/iphone-to-dslr-flower-test-random.html">Random test set results</a>
    </p></td>
  </tr>
  <tr>
      <td align="center" valign="middle"><a href="../CycleGAN Project Page_files/photo_enhancement.jpg"><img src="../CycleGAN Project Page_files/photo_enhancement.jpg" width="1000"> </a></td>
  </tr>
</tbody></table>
<p>&nbsp;</p>





<h2>Experiments and comparisons</h2>
<ul id="comparison">
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/cityscapes-comparison.html"> Comparison on Cityscapes</a>: different methods for mapping labels ↔ photos trained on Cityscapes.</li>
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/maps-comparison.html"> Comparison on Maps</a>: different methods for mapping aerialphotos ↔ maps on Google Maps.</li>
  <li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/facades.html"> Facade results</a>: CycleGAN for mapping labels ↔ facades on <a href="http://cmp.felk.cvut.cz/~tylecr1/facade/">CMP</a> Facades datasets.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/cityscapes-ablation.html">Ablation studies</a>: different variants of our method for mapping labels ↔ photos trained on Cityscapes.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/reconstructed-images.html">Image reconstruction results</a>:  the reconstructed  images F(G(x)) and G(F(y)) from  various experiments.</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/gatys-comparison.html">Style transfer comparison</a>:  we compare our method with neural style transfer [Gatys et al. '15].</li>
<li font-size:="" 15px=""><a href="https://taesungp.github.io/cyclegan/2017/03/25/monet-to-photo-idt-comparison.html">Identity mapping loss</a>:  the effect of the identity mapping loss on Monet to Photo.</li>
</ul>

<h2>Failure Cases</h2>
<a href="../CycleGAN Project Page_files/failure_putin.jpg"><img style="float: left;  PADDING-RIGHT: 30px;" alt="failure" src="../CycleGAN Project Page_files/failure_putin.jpg" width="350"></a>
<p align="justify">Our model does not work well when a test image looks unusual compared to training images, as shown in the left figure. See more typical failure cases <a href="https://junyanz.github.io/CycleGAN/images/failures.jpg">[here]</a>. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of dog ↔ cat transfiguration, the learned translation degenerates into making minimal changes to the input. Handling more varied and extreme transformations, especially geometric changes, is an important problem for future work. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible, -- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos → labels task. To resolve this ambiguity may require some form of weak semantic supervision. Integrating weak or semi-supervised data may lead to substantially more powerful translators, still at a fraction of the annotation cost of the fully-supervised systems.
</p>


    <h2 style="text-align: center;">Meet the Authors of CycleGAN</h2>

<p></p>
<p align="center"> <iframe width="560" height="315" src="../CycleGAN Project Page_files/05zzhkyofLE.html" frameborder="0" align="middle" allowfullscreen=""></iframe></p>
-->
<h1>Related Work</h1>
<ul id="relatedwork">
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1905.12887"><strong>Does computer vision matter for action?</strong></a>, Brady Zhou, Philipp Krähenbühl, and Vladlen Koltun, Science Robotics 2019.
</li>
<li font-size:="" 14px=""><a href="https://arxiv.org/abs/1812.11971"><strong>Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies</strong></a>, Alexander Sax, Bradley Emi, Amir R. Zamir, Leonidas Guibas, Silvio Savarese, and Jitendra Malik, CoRL 2019.
</li>
</ul>

<h1>Acknowledgement</h1>
<p align="justify">We thank Kevin Zakka and Jimmy Wu for all the good meals together.</p>

<div style="display:none">
<script type="text/javascript" src="../CycleGAN Project Page_files/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>

</div></body></html>